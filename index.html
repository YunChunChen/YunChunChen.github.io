<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
  <title>Yun-Chun Chen</title>
  <link rel="icon" type="image/png" href="img/yc.jpg"/>

  <!-- CSS  -->
  <link href="css/materialize.min.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/aos.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/font-awesome.min.css" rel="stylesheet" >
  <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection"/>

</head>
<body>
  
  <div class="navbar-fixed">
    <nav class="">
      <div class="nav-wrapper container"><a id="logo-container" href="#" class="brand-logo"></a>
        <ul class="left">
          <li><a class="nav-item waves-effect waves-light" href="#home">Home</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#about">Bio</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#news">News</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#publication">Publications</a></li>
        </ul>
      </div>
    </nav>
  </div>

<div id="home" class="parallax-container scrollspy">

  <div class="container row cover-block">

    <div class="profile-image-block col s12 m12 l4 center">
        <img class="responsive-img profile-photo z-depth-2" src="img/yc.jpg">
    </div>

    <div class="profile-content-block col s12 m12 l8">
        <h5 class="profile-name">Yun-Chun Chen</h5>
        <hr>
        <br>
        <h6 class="profile-link"><a href="https://www.utoronto.ca">University of Toronto</a></h6 class="profile-link">
        <br>
        <h6 class="profile-link"><a href="https://research.adobe.com/">Adobe Research</a></h6 class="profile-link">
        <br>
        <h6 class="profile-link"><b>Email</b>: <a href="mailto:ycchen@cs.toronto.edu">ycchen@cs.toronto.edu</a></h6 class="profile-link">
        <br>
        <h6 class="profile-link">
        <a href="CV/Yun-Chun_Chen_CV.pdf" style="color:white;font-size:14pt">CV</a> &nbsp; | &nbsp;
        <a href="https://github.com/YunChunChen" style="color:white;font-size:14pt">GitHub</a> &nbsp; | &nbsp; 
        <a href="https://scholar.google.com/citations?user=TiCSofEAAAAJ&hl=en" style="color:white;font-size:14pt">Google Scholar</a> &nbsp; | &nbsp;
        <a href="https://www.linkedin.com/in/ycchen918/" style="color:white;font-size:14pt">LinkedIn</a> &nbsp; | &nbsp; 
        <a href="https://twitter.com/ycchen918" style="color:white;font-size:14pt">Twitter</a>
        </h6>
    </div>
  </div>
  <div class="parallax"><img src="img/cover_blur.png" alt="Unsplashed background img 1"></div>
</div>


<div class="section about-section scrollspy" id="about">
  <div class="row container">
    <br>
    <div class="row">
      <div class="title">Bio</div>
      <hr>
    </div>
    <div class="row">

        <p>I am a Ph.D. student in <a href="https://web.cs.toronto.edu">Computer Science</a> at the <a href="https://www.utoronto.ca">University of Toronto</a>, advised by <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a>. I also work at <a href="https://research.adobe.com/">Adobe Research</a> with <a href="http://www.vovakim.com/">Vova Kim</a> and <a href="http://mgadelha.me/">Matheus Gadelha</a>.</p>

        <p>In 2022, I worked at <a href="https://research.adobe.com/">Adobe Research</a> with <a href="http://www.vovakim.com/">Vova Kim</a> and <a href="https://noamaig.github.io/">Noam Aigerman</a>. In 2021, I was an intern in the <a href="https://nvidia_srl.gitlab.io/">NVIDIA Seattle Robotics</a> lab, working with <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a>, <a href="http://adithyamurali.com/">Adithya Murali</a>, and <a href="https://balakumar-s.github.io/">Balakumar Sundaralingam</a>.</p>

        <p>I am interested in computer vision, computer graphics, 3D geometry, and geometric deep learning.</p>

        <p>Prior to my Ph.D., I worked with <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>, and <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>. I received a B.S. in <a href="https://web.ee.ntu.edu.tw/eng/index.php">Electrical Engineering</a> from <a href="http://www.ntu.edu.tw/english/">National Taiwan University</a> in 2018.</p>

    </div>
    
  </div>
</div>

<div class="section news-section scrollspy" id="news">
  <div class="row container">
    <br>
    <div class="row">
      <div class="title">News</div>
      <hr>
    </div>
    <div class="row">
      <ul>
      	<li>06 / 2023: &nbsp; Start my internship at <a href="https://research.adobe.com/">Adobe Research</a>.</li>
      	<li>04 / 2023: &nbsp; Passed the qualifying oral exam!</li>
      	<li>03 / 2023: &nbsp; One paper on progressive representations for meshes accepted to <a href="https://s2023.siggraph.org/">SIGGRAPH 2023</a>.</li>
        <li>11 / 2022: &nbsp; <a href="">Breaking Bad</a> is selected as a <b>featured paper presentation</b> at <a href="https://nips.cc/Conferences/2022">NeurIPS 2022</a>.</li>
        <li>10 / 2022: &nbsp; I am serving as a mentor for <a href="https://sites.google.com/view/torontogaap">Toronto GAAP</a> and <a href="https://research.siggraph.org/programs/undergraduate-mentoring/">RCDC@SIGGRAPH</a>.</li>
      	<li>09 / 2022: &nbsp; One paper on <a href="https://arxiv.org/abs/2210.11463">geometric shape assembly dataset</a> is accepted to <a href="https://nips.cc/Conferences/2022">NeurIPS 2022</a>.</li>
      	<li>07 / 2022: &nbsp; One <b>oral</b> paper on <a href="https://arxiv.org/abs/2208.12250">multi-finger grasp synthesis with differentiable simulation</a> is accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.</li>
      	<li>06 / 2022: &nbsp; Start my internship at <a href="https://research.adobe.com/">Adobe Research</a>.</li>
      	<li>05 / 2022: &nbsp; One paper on <a href="https://arxiv.org/abs/2206.14854">implicit representations for motion planning</a> is accepted to <a href="https://imrss2022.github.io/">RSS 2022 Workshop</a>.</li>
      	<li>03 / 2022: &nbsp; One paper on <a href="https://arxiv.org/abs/2205.14886">3D geometric shape assembly</a> is accepted to <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>.</li>
      	<li>10 / 2021: &nbsp; One paper on <a href="https://arxiv.org/abs/2103.14182">3D human pose and shape estimation</a> is accepted to <a href="https://www.journals.elsevier.com/computer-vision-and-image-understanding">CVIU 2021</a>.</li>
      	<li>06 / 2021: &nbsp; One paper on <a href="https://arxiv.org/abs/2101.07241">visual imitation learning</a> is accepted to <a href="https://www.iros2021.org/">IROS 2021</a>.</li>
        <li>05 / 2021: &nbsp; Start my internship in the <a href="https://nvidia_srl.gitlab.io/">NVIDIA Seattle Robotics Lab</a>.</li>
        <li>05 / 2021: &nbsp; I am selected as the <a href="https://aaai.org/Conferences/AAAI-21/wp-content/uploads/2021/05/AAAI-21-Program-Committee.pdf">Top 25% of Program Committee Members</a> of <a href="https://aaai.org/Conferences/AAAI-21/">AAAI 2021</a>.</li>
        <li>09 / 2020: &nbsp; Start my Ph.D. at the <a href="https://www.utoronto.ca">University of Toronto</a> and the <a href="https://vectorinstitute.ai">Vector Institute</a>. </li>
        <li>07 / 2020: &nbsp; Two papers on <a href="https://arxiv.org/abs/2008.11713">neural architecture search</a> and <a href="https://arxiv.org/abs/2008.11203">meta-learning</a> are accepted to <a href="https://eccv2020.eu/">ECCV 2020</a>.</li>
        <li>03 / 2020: &nbsp; One paper on <a href="https://arxiv.org/abs/1906.05857">joint semantic matching and object co-segmentation</a> is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">PAMI 2021</a>.</li>
        <li>07 / 2019: &nbsp; One paper on <a href="https://arxiv.org/abs/1908.06052">cross-resolution generative modeling</a> is accepted to <a href="https://iccv2019.thecvf.com/">ICCV 2019</a>.</li>
        <li>02 / 2019: &nbsp; One paper on <a href="https://arxiv.org/abs/2001.03182">unsupervised domain adaptation</a> is accepted to <a href="https://cvpr2019.thecvf.com/">CVPR 2019</a>.</li>
        <li>11 / 2018: &nbsp; One <b>oral</b> paper on <a href="https://arxiv.org/abs/1907.10843">representation learning</a> is accepted to <a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a>.</li>
        <li>10 / 2018: &nbsp; We won the <b>Third Place</b> in <a href="https://2018.ieeeicip.org/VIPCup.asp">IEEE Video and Image Processing (VIP) Cup</a>.</li>
        <li>07 / 2018: &nbsp; One paper on <a href="https://arxiv.org/abs/2004.00144">semantic matching</a> is accepted to <a href="">ACCV 2018</a>.</li>
      </ul>
    </div>
  </div>
</div>

<div class="section publication-section scrollspy" id="publication">
  <div class="row container">
    <br>
    <div class="row">
      <div class="title">Selected Publications</div>
      <hr>
    </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">A Multiresolution Dataset of Self-Consistent Cloth Drapes for Physics-Based Upsampling</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>, 
            <a href="https://eriszhang.github.io/">Jiayi Eris Zhang</a><sup>*</sup>, 
            <a href="https://dannykaufman.io/">Danny M. Kaufman</a>, 
            <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a>
          </div>
          <font color="black">Under review at Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2023</font> 
          <br />
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Neural Progressive Meshes</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>, 
            <a href="http://www.vovakim.com/">Vladimir G. Kim</a>, 
            <a href="https://noamaig.github.io/">Noam Aigerman</a>, 
            <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a>
          </div>
          <font color="black">ACM SIGGRAPH, 2023</font> 
          <br />
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/NeurIPS-dataset-22/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Breaking Bad: A Dataset for Geometric Fracture and Reassembly</div>
          <div class="paper-author"> 
            <a href="https://www.silviasellan.com/">Silvia Sellán</a><sup>*</sup>,
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>,
            <a href="https://wuziyi616.github.io/">Ziyi Wu</a><sup>*</sup>,
            <a href="">Animesh Garg</a>, 
            <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a>
          </div>
          <font color="black">Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2022</font> 
          <br />
          <font color="red">Featured Paper Presentation</font> 
          <br />
          [<a href="https://arxiv.org/abs/2210.11463">Paper</a>]
          [<a href="https://breaking-bad-dataset.github.io/">Project page</a>]
          [<a href="https://github.com/Wuziyi616/multi_part_assembly">Baseline code</a>]
          [<a href="https://github.com/sgsellan/fracture-modes#dataset">Data generation code</a>]
          [<a href="https://borealisdata.ca/dataset.xhtml?persistentId=doi:10.5683/SP3/LZNPKB">Dataset</a>]
          [<a href="">Video</a>]
          [<a href="papers/NeurIPS-dataset-22/poster.pdf">Poster</a>]
          [<a href="papers/NeurIPS-dataset-22/slides.pptx">Slides</a>]
          [<a href="https://twitter.com/ycchen918/status/1586169332685471745">Twitter</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/ECCV-22/teaser.gif">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Grasp'D: Differentiable Contact-rich Grasp Synthesis for Multi-fingered Hands</div>
          <div class="paper-author"> 
            <a href="http://www.cs.toronto.edu/~dylanturpin/">Dylan Turpin</a>, 
            <a href="https://www.linkedin.com/in/liquan-wang-a37634196/?originalSubdomain=ca">Liquan Wang</a>, 
            <a href="https://eric-heiden.com/">Eric Heiden</a>, 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>, 
            <a href="http://blog.mmacklin.com/about/">Miles Macklin</a>, 
            <a href="http://tsogkas.github.io/">Stavros Tsogkas</a>, 
            <a href="https://www.cs.toronto.edu/~sven/">Sven Dickinson</a>, 
            <a href="">Animesh Garg</a>
          </div>
          <font color="black">European Conference on Computer Vision (ECCV), 2022</font> 
          <br />
          <font color="red">Oral Presentation</font> 
          <br />
          [<a href="https://arxiv.org/abs/2208.12250">Paper</a>]
          [<a href="https://graspd-eccv22.github.io/">Project page</a>]
          [<a href="https://github.com/dylanturpin/graspd">Code</a>]
          [<a href="https://www.youtube.com/watch?v=tkl_lywZ94o">Video</a>]
          [<a href="papers/ECCV-22/poster.pdf">Poster</a>]
          [<a href="https://twitter.com/dylanturpin/status/1567151353910132737">Twitter</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/RSS-W-22/teaser.gif">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Neural Motion Fields: Encoding Grasp Trajectories as Implicit Value Functions</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>,
            <a href="http://adithyamurali.com/">Adithyavairavan Murali</a><sup>*</sup>,
            <a href="https://balakumar-s.github.io/">Balakumar Sundaralingam</a><sup>*</sup>,
            <a href="http://wyang.me/">Wei Yang</a>,
            <a href="">Animesh Garg</a>, 
            <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a>
          </div>
          <font color="black">RSS 2022 Workshop on Implicit Representations for Robotic Manipulation, 2022</font> 
          <br />
          <font color="red">Spotlight Talk</font> 
          <br />
          [<a href="https://arxiv.org/abs/2206.14854">Paper</a>]
          [<a href="https://youtu.be/B-pEhT1pi-Q">Video</a>]
          [<a href="papers/RSS-W-22/slides.pptx">Slides</a>]
          [<a href="https://twitter.com/ycchen918/status/1542914734332182531">Twitter</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/CVPR-22/teaser.jpeg">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Neural Shape Mating: Self-Supervised Object Assembly with Adversarial Shape Priors</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="https://lihd1003.github.io/">Haoda Li</a>,
            <a href="http://www.cs.toronto.edu/~dylanturpin/">Dylan Turpin</a>,
            <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a>, 
            <a href="">Animesh Garg</a>
          </div>
          <font color="black">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022</font> 
          <br />
          [<a href="https://arxiv.org/abs/2205.14886">Paper</a>]
          [<a href="https://neural-shape-mating.github.io/">Project page</a>]
          [<a href="https://youtu.be/KVvTpGymEbU?t=110">Video</a>]
          [<a href="papers/CVPR-22/poster.pdf">Poster</a>]
          [<a href="papers/CVPR-22/slides.pptx">Slides</a>]
          [<a href="https://twitter.com/animesh_garg/status/1539036306734346240">Twitter</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/IROS-21/teaser.gif">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</div>
          <div class="paper-author"> 
            <a href="https://haoyu-x.github.io/">Haoyu Xiong</a>,
            <a href="https://quanzhou-li.github.io/">Quanzhou Li</a>,
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="https://homangab.github.io/">Homanga Bharadhwaj</a>,
            <a href="https://www.samsinha.me/">Samarth Sinha</a>, 
            <a href="">Animesh Garg</a> <br />
          </div>
          <font color="black">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021</font> 
          <br />
          <font color="black">RSS 2021 Workshop on Visual Learning and Reasoning for Robotics, 2021 <font color="red">Spotlight Talk</font></font> 
          <br />
          <font color="black">ICML 2021 Workshop on Human in the Loop Learning, 2021</font> 
          <br />
          [<a href="https://arxiv.org/abs/2101.07241">Paper</a>]
          [<a href="http://www.pair.toronto.edu/lbw-kp/">Project page</a>]
          [<a href="https://youtu.be/Retu1q-BbEo">Video</a>]
          [<a href="https://twitter.com/HaoyuXiong1/status/1351581941603004418">Twitter</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/PAMI-20/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Show, Match and Segment: Joint Weakly Supervised Learning of Semantic Matching and Object Co-segmentation</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>,
            <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, 
            <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>
          </div>
          <font color="black">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2021</font> 
          <br />
          [<a href="https://arxiv.org/abs/1906.05857">Paper</a>]
          [<a href="https://yunchunchen.github.io/MaCoSNet-web/">Project page</a>]
          [<a href="https://github.com/YunChunChen/MaCoSNet-pytorch">Code</a>]
          [<a href="papers/PAMI-20/slides.pdf">Slides</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/CVIU-21/teaser.gif">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Self-Attentive 3D Human Pose and Shape Estimation from Videos</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="https://mpicci.github.io/">Marco Piccirilli</a>,
            <a href="https://www.linkedin.com/in/rpiramuthu/">Robinson Piramuthu</a>, 
            <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
          </div>
          <font color="black">Computer Vision and Image Understanding (CVIU), 2021</font> 
          <br /> 
          [<a href="https://arxiv.org/abs/2103.14182">Paper</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/ECCV-20/NAS-DIP/teaser.gif">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">NAS-DIP: Learning Deep Image Prior with Neural Architecture Search</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>,
            <a href="https://gaochen315.github.io">Chen Gao</a><sup>*</sup>,
            <a href="http://estherrobb.com">Esther Robb</a>, 
            <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>
          </div>
          <font color="black">European Conference on Computer Vision (ECCV), 2020</font> 
          <br /> 
          [<a href="https://arxiv.org/abs/2008.11713">Paper</a>]
          [<a href="https://yunchunchen.github.io/NAS-DIP/">Project page</a>]
          [<a href="https://github.com/YunChunChen/NAS-DIP-pytorch">GitHub</a>]
          [<a href="https://colab.research.google.com/drive/1BhmZMeyGGP_T5SLPdLlkGUZLhrnO1FdF?usp=sharing">Colab</a>]
          [<a href="papers/ECCV-20/NAS-DIP/highlight.mp4">Highlight video</a>]
          [<a href="papers/ECCV-20/NAS-DIP/highlight.pdf">Highlight slides</a>]
          [<a href="papers/ECCV-20/NAS-DIP/full.mp4">Full video</a>]
          [<a href="papers/ECCV-20/NAS-DIP/full.pdf">Full slides</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/ECCV-20/Meta/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Learning to Learn in a Semi-Supervised Fashion</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="">Chao-Te Chou</a>, 
            <a href="">Yu-Chiang Frank Wang</a>
          </div>
          <font color="black">European Conference on Computer Vision (ECCV), 2020</font> 
          <br /> 
          [<a href="https://arxiv.org/abs/2008.11203">Paper</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/arXiv-20/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Cross-Resolution Adversarial Dual Network for Person Re-Identification and Beyond</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>,
            <a href="https://yujheli.github.io">Yu-Jhe Li</a><sup>*</sup>,
            <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>, 
            <a href="">Yu-Chiang Frank Wang</a>
          </div>
          <font color="black">arXiv preprint arXiv:2002.09274</font> 
          <br /> 
          [<a href="https://arxiv.org/abs/2002.09274">Paper</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/ICCV-19/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>,
            <a href="https://yujheli.github.io">Yu-Jhe Li</a><sup>*</sup>,
            <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>,
            <a href="">Xiaofei Du</a>, 
            <a href="">Yu-Chiang Frank Wang</a>
          </div>
          <font color="black">IEEE International Conference on Computer Vision (ICCV), 2019</font> 
          <br />
          [<a href="https://arxiv.org/abs/1908.06052">Paper</a>]
          [<a href="papers/ICCV-19/slides.pptx">Slides</a>]
          [<a href="papers/ICCV-19/poster.pdf">Poster</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/CVPR-19/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>,
            <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, 
            <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>
          </div>
          <font color="black">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019</font> 
          <br />
          [<a href="https://arxiv.org/abs/2001.03182">Paper</a>]
          [<a href="https://yunchunchen.github.io/CrDoCo/">Project page</a>]
          [<a href="https://github.com/YunChunChen/CrDoCo-pytorch">Code</a>]
          [<a href="papers/CVPR-19/slides.pptx">Slides</a>]
          [<a href="papers/CVPR-19/poster.pdf">Poster</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/AAAI-19/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Learning Resolution-Invariant Deep Representations for Person Re-Identification</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>,
            <a href="https://yujheli.github.io">Yu-Jhe Li</a><sup>*</sup>,
            <a href="">Xiaofei Du</a>, 
            <a href="">Yu-Chiang Frank Wang</a>
          </div>
          <font color="black">AAAI Conference on Artificial Intelligence (AAAI), 2019</font> 
          <br />
          <font color="red">Oral Presentation</font> 
          <br />
          [<a href="https://arxiv.org/abs/1907.10843">Paper</a>]
          [<a href="papers/AAAI-19/slides.pptx">Slides</a>]
          [<a href="papers/AAAI-19/poster.pdf">Poster</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/ACCV-18/teaser.jpg">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Deep Semantic Matching with Foreground Detection and Cycle-Consistency</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="">Po-Hsiang Huang</a>,
            <a href="">Li-Yu Yu</a>,
            <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>,
            <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, 
            <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>
          </div>
          <font color="black">Asian Conference on Computer Vision (ACCV), 2018</font> 
          <br />
          [<a href="https://arxiv.org/abs/2004.00144">Paper</a>]
          [<a href="https://yunchunchen.github.io/WeakMatchNet/">Project page</a>]
          [<a href="https://github.com/YunChunChen/WeakMatchNet">Code</a>]
          [<a href="papers/ACCV-18/poster.pdf">Poster</a>]
        </div>
      </div>

  </div>
</div>

<footer class="page-footer white lighten-4">
    <div class="row">
      <div class="col l4 offset-l4 s12">
        <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=THulLRlsDUvjodnYpDCviW_mAVbauRC9pS7yOCczJQI"></script>
      </div>
    </div>
    <div class="footer-copyright center black-text">
      Template from <a href="http://www.wslai.net">Jason Lai</a>
    </div>
  
</footer>

<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="js/materialize.js"></script>
<script src="js/aos.js"></script>
<script src="js/init.js"></script>

</body>
</html>
