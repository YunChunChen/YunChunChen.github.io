<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
  <title>Yun-Chun Chen</title>
  <link rel="icon" type="image/png" href="img/yc.jpg"/>

  <!-- CSS  -->
  <link href="css/materialize.min.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/aos.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/font-awesome.min.css" rel="stylesheet" >
  <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection"/>

</head>
<body>
  
  <div class="navbar-fixed">
    <nav class="">
      <div class="nav-wrapper container"><a id="logo-container" href="#" class="brand-logo"></a>
        <ul class="left">
          <li><a class="nav-item waves-effect waves-light" href="#home">Home</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#about">Bio</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#news">News</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#publication">Publications</a></li>
        </ul>
      </div>
    </nav>
  </div>

<div id="home" class="parallax-container scrollspy">

  <div class="container row cover-block">

    <div class="profile-image-block col s12 m12 l4 center">
        <img class="responsive-img profile-photo z-depth-2" src="img/yc.jpg">
    </div>

    <div class="profile-content-block col s12 m12 l8">
        <h5 class="profile-name">Yun-Chun Chen</h5>
        <hr>
        <br>
        <h6 class="profile-link"><a href="https://www.utoronto.ca">University of Toronto</a></h6 class="profile-link">
        <br>
        <h6 class="profile-link"><a href="https://vectorinstitute.ai">Vector Institute</a></h6 class="profile-link">
        <br>
        <h6 class="profile-link"><a href="https://research.adobe.com/">Adobe Research</a></h6 class="profile-link">
        <br>
        <h6 class="profile-link"><b>Email</b>: <a href="mailto:ycchen@cs.toronto.edu">ycchen@cs.toronto.edu</a></h6 class="profile-link">
        <br>
        <h6 class="profile-link">
        <a href="CV/CV.pdf" style="color:white;font-size:14pt">CV</a> &nbsp; | &nbsp;
        <a href="https://github.com/YunChunChen" style="color:white;font-size:14pt">GitHub</a> &nbsp; | &nbsp; 
        <a href="https://scholar.google.com/citations?user=TiCSofEAAAAJ&hl=en" style="color:white;font-size:14pt">Google Scholar</a> &nbsp; | &nbsp;
        <a href="https://www.linkedin.com/in/ycchen918/" style="color:white;font-size:14pt">LinkedIn</a> &nbsp; | &nbsp; 
        <a href="https://twitter.com/ycchen918" style="color:white;font-size:14pt">Twitter</a>
        </h6>
    </div>
  </div>
  <div class="parallax"><img src="img/cover_blur.png" alt="Unsplashed background img 1"></div>
</div>


<div class="section about-section scrollspy" id="about">
  <div class="row container">
    <br>
    <div class="row">
      <div class="title">Bio</div>
      <hr>
    </div>
    <div class="row">
        <p>I am a Ph.D. student in <a href="https://web.cs.toronto.edu">Computer Science</a> at the <a href="https://www.utoronto.ca">University of Toronto</a>, advised by <a href="https://animesh.garg.tech">Animesh Garg</a> and <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a>. I am part of the <a href="https://robotics.cs.toronto.edu/index.html">CS Robotics</a> group and <a href="https://www.dgp.toronto.edu/">DGP</a> and am affiliated with the <a href="https://vectorinstitute.ai">Vector Institute</a>, <a href="https://robotics.utoronto.ca/">UofT Robotics Institute</a>, and <a href="https://datasciences.utoronto.ca/">UofT Data Sciences Institute</a>.</p>
        
        <p>I also work at <a href="https://research.adobe.com/">Adobe Research</a> with <a href="http://www.vovakim.com/">Vova Kim</a> and <a href="https://noamaig.github.io/">Noam Aigerman</a>. In 2021, I was an intern on the <a href="https://www.nvidia.com/en-us/research/robotics/">Robotics</a> team at <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>, working with <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a>, <a href="http://adithyamurali.com/">Adithya Murali</a>, and <a href="https://balakumar-s.github.io/">Balakumar Sundaralingam</a>.</p>

        <p>I am interested in computer vision, computer graphics, geometry processing, and robotics.</p>

        <p>Prior to my Ph.D., I worked with <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>, and <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>. I received a B.S. in <a href="https://web.ee.ntu.edu.tw/eng/index.php">Electrical Engineering</a> from <a href="http://www.ntu.edu.tw/english/">National Taiwan University</a> in 2018.</p>
    </div>
  </div>
</div>

<div class="section news-section scrollspy" id="news">
  <div class="row container">
    <br>
    <div class="row">
      <div class="title">News</div>
      <hr>
    </div>
    <div class="row">
      <ul>
      	<li>06 / 2022: &nbsp; One oral paper on multi-finger grasping with differentiable simulation is accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.</li>
        <li>06 / 2022: &nbsp; I am serving as a reviewer for <a href="https://neurips.cc/Conferences/2022/CallForDatasetsBenchmarks">NeurIPS 2022 Datasets and Benchmarks</a>, <a href="https://nips.cc/Conferences/2022">NeurIPS 2022</a>, <a href="https://icml.cc/Conferences/2022">ICML 2022</a>, <a href="https://iclr.cc/Conferences/2022">ICLR 2022</a>, <a href="https://sa2022.siggraph.org/en/">SIGGRAPH Asia 2022</a>, <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>, <a href="https://eccv2022.ecva.net/">ECCV 2022</a>, <a href="https://3dvconf.github.io/2022/">3DV 2022</a>, <a href="http://www.accv2022.org/">ACCV 2022</a>, and <a href="https://h2t-projects.webarchiv.kit.edu/ISRR2022/">ISRR 2022</a>.</li>
      	<li>06 / 2022: &nbsp; I am serving as a program committee for <a href="https://wacv2023.thecvf.com">WACV 2023</a>.</li>
      	<li>06 / 2021: &nbsp; Start my internship at <a href="https://research.adobe.com/">Adobe Research</a>.</li>
      	<li>05 / 2022: &nbsp; One paper on <a href="https://arxiv.org/abs/2206.14854">implicit representations for motion planning</a> is accepted to <a href="https://imrss2022.github.io/">RSS 2022 Workshop on Implicit Representations for Robotic Manipulation</a>.</li>
      	<li>03 / 2022: &nbsp; I will be presenting <a href="https://arxiv.org/abs/2205.14886">Neural Shape Mating</a> at the <a href="https://toronto-geometry-colloquium.github.io/">Toronto Geometry Colloquium</a>.</li>
      	<li>03 / 2022: &nbsp; One paper on <a href="https://arxiv.org/abs/2205.14886">3D geometric shape assembly</a> is accepted to <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>.</li>
      	<li>01 / 2022: &nbsp; I am working as a teaching assistant for <a href="https://www.pair.toronto.edu/csc375-w22/">CSC 375: Algorithmic Intelligence in Robotics</a> and <a href="https://uoft-csc413.github.io/2022/">CSC 413/2516: Neural Networks and Deep Learning</a>. </li>
      	<li>10 / 2021: &nbsp; I am serving as a reviewer for <a href="https://www.ieee-ras.org/publications/ra-l">IEEE Robotics and Automation Letters</a>, <a href="https://www.journals.elsevier.com/image-and-vision-computing">Image and Vision Computing</a>, and <a href="https://ietresearch.onlinelibrary.wiley.com/journal/17519640">IET Computer Vision</a>.</li>
      	<li>10 / 2021: &nbsp; I am serving as a program committee for <a href="https://ijcai-22.org/">IJCAI 2022</a>, <a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a>, and <a href="http://wacv2022.thecvf.com/">WACV 2022</a>.</li>
      	<li>10 / 2021: &nbsp; One paper on <a href="https://arxiv.org/abs/2103.14182">3D Human Pose and Shape Estimation</a> is accepted to <a href="https://www.journals.elsevier.com/computer-vision-and-image-understanding">CVIU 2021</a>.</li>
      	<li>06 / 2021: &nbsp; One paper on <a href="https://arxiv.org/abs/2101.07241">visual imitation learning</a> is accepted to <a href="https://www.iros2021.org/">IROS 2021</a>.</li>
      	<li>06 / 2021: &nbsp; I am serving as a volunteer for <a href="https://iclr.cc/Conferences/2021">ICLR 2021</a> and <a href="https://icml.cc/Conferences/2021">ICML 2021</a>.</li>
        <li>05 / 2021: &nbsp; Start my internship on the <a href="https://www.nvidia.com/en-us/research/robotics/">Robotics</a> team at <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>.</li>
        <li>05 / 2021: &nbsp; I am selected as the <a href="https://aaai.org/Conferences/AAAI-21/wp-content/uploads/2021/05/AAAI-21-Program-Committee.pdf">Top 25% of Program Committee Members</a> of <a href="https://aaai.org/Conferences/AAAI-21/">AAAI 2021</a>.</li>
        <li>04 / 2021: &nbsp; I am serving as a program committee for <a href="https://nips.cc/Conferences/2021">NeurIPS 2021</a>, <a href="https://aaai.org/Conferences/AAAI-21/">AAAI 2021</a>, <a href="http://wacv2021.thecvf.com/home">WACV 2021</a>, and <a href="https://www.grc.org/">Gordon Research Conference/Seminar in Robotics, 2022</a>.</li>
        <li>01 / 2021: &nbsp; I am serving as a reviewer for <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>, <a href="https://icml.cc/Conferences/2021">ICML 2021</a>, <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>, <a href="https://iclr.cc/Conferences/2021">ICLR 2021</a>, <a href="http://www.icra2021.org/">ICRA 2021</a>, and <a href="https://www.bmvc2021.com/">BMVC 2021</a>.</li>
        <li>01 / 2021: &nbsp; I am working as a teaching assistant for <a href="https://csc413-uoft.github.io/2021/">CSC 413/2516: Neural Networks and Deep Learning</a>. </li>
        <li>12 / 2020: &nbsp; I am serving as a reviewer for <a href="https://www.springer.com/journal/11263">International Journal of Computer Vision</a>. </li>
        <li>10 / 2020: &nbsp; I am serving as a senior program committee for <a href="https://ijcai-21.org/">IJCAI 2021</a>.</li>
        <li>09 / 2020: &nbsp; Start my Ph.D. at the <a href="https://www.utoronto.ca">University of Toronto</a> and the <a href="https://vectorinstitute.ai">Vector Institute</a>. </li>
        <li>09 / 2020: &nbsp; I am serving as a reviewer for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing</a>. </li>
        <li>08 / 2020: &nbsp; I am serving as a reviewer for <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>, <a href="https://eccv2020.eu/">ECCV 2020</a>, <a href="https://nips.cc/Conferences/2020">NeurIPS 2020</a>, <a href="https://www.robot-learning.org/">CoRL 2020</a>, <a href="https://www.bmvc2020-conference.com/">BMVC 2020</a>, and <a href="https://accv2020.github.io/">ACCV 2020</a>.</li>
        <li>07 / 2020: &nbsp; Two papers on <a href="https://arxiv.org/abs/2008.11713">neural architecture search</a> and <a href="https://arxiv.org/abs/2008.11203">meta-learning</a> are accepted to <a href="https://eccv2020.eu/">ECCV 2020</a>.</li>
        <li>03 / 2020: &nbsp; One paper on <a href="https://arxiv.org/abs/1906.05857">joint semantic matching and object co-segmentation</a> is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">PAMI 2021</a>.</li>
        <li>09 / 2019: &nbsp; I am serving as a program committee for <a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a>.</li>
        <li>07 / 2019: &nbsp; One paper on <a href="https://arxiv.org/abs/1908.06052">cross-resolution generative modeling</a> is accepted to <a href="https://iccv2019.thecvf.com/">ICCV 2019</a>.</li>
        <li>03 / 2019: &nbsp; I am serving as a reviewer for <a href="https://iccv2019.thecvf.com/">ICCV 2019</a>, <a href="https://bmvc2019.org/">BMVC 2019</a>, and <a href="https://www.2019.ieeeicip.org/2019.ieeeicip.org/index.html">ICIP 2019</a>.</li>
        <li>02 / 2019: &nbsp; One paper on <a href="https://arxiv.org/abs/2001.03182">unsupervised domain adaptation</a> is accepted to <a href="https://cvpr2019.thecvf.com/">CVPR 2019</a>.</li>
        <li>11 / 2018: &nbsp; One oral paper on <a href="https://arxiv.org/abs/1907.10843">representation learning</a> is accepted to <a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a>.</li>
        <li>10 / 2018: &nbsp; We won the <b>Third Place</b> in <a href="https://2018.ieeeicip.org/VIPCup.asp">IEEE Video and Image Processing (VIP) Cup</a>.</li>
        <li>07 / 2018: &nbsp; One paper on <a href="https://arxiv.org/abs/2004.00144">semantic matching</a> is accepted to <a href="">ACCV 2018</a>.</li>
      </ul>
    </div>
  </div>
</div>

<div class="section publication-section scrollspy" id="publication">
  <div class="row container">
    <br>
    <div class="row">
      <div class="title">Selected Publications</div>
      <hr>
    </div>
      
      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/NeurIPS-dataset-22/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Breaking Bad: A Dataset for Geometric Fracture and Reassembly</div>
          <div class="paper-author"> 
            <a href="https://www.silviasellan.com/">Silvia Sellán</a><sup>*</sup>,
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>,
            <a href="https://wuziyi616.github.io/">Ziyi Wu</a><sup>*</sup>,
            <a href="https://animesh.garg.tech/">Animesh Garg</a>, and
            <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a> <br />
          </div>
          <div class="paper-conf">Under review at Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2022<br /></div> 
          [<a href="">Paper</a>]
          [<a href="">Project page</a>]
          [<a href="">Video</a>]
          [<a href="">Poster</a>]
          [<a href="">Slides</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/ECCV-22/teaser.gif">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Grasp'D: Differentiable Contact-rich Grasp Synthesis for Multi-fingered Hands</div>
          <div class="paper-author"> 
            <a href="http://www.cs.toronto.edu/~dylanturpin/">Dylan Turpin</a>, 
            <a href="https://www.linkedin.com/in/liquan-wang-a37634196/?originalSubdomain=ca">Liquan Wang</a>, 
            <a href="https://eric-heiden.com/">Eric Heiden</a>, 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>, 
            <a href="http://blog.mmacklin.com/about/">Miles Macklin</a>, 
            <a href="http://tsogkas.github.io/">Stavros Tsogkas</a>, 
            <a href="https://www.cs.toronto.edu/~sven/">Sven Dickinson</a>, and
            <a href="https://animesh.garg.tech/">Animesh Garg</a>
          </div>
          <br />
          <font color="black">European Conference on Computer Vision (ECCV), 2022</font> 
          <br />
          <font color="red">Oral Presentation</font> 
          <br />
          [<a href="">Paper</a>]
          [<a href="">Project page</a>]
          [<a href="">Video</a>]
          [<a href="">Poster</a>]
          [<a href="">Slides</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/RSS-W-22/teaser.gif">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Neural Motion Fields: Encoding Grasp Trajectories as Implicit Value Functions</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>,
            <a href="http://adithyamurali.com/">Adithyavairavan Murali</a><sup>*</sup>,
            <a href="https://balakumar-s.github.io/">Balakumar Sundaralingam</a><sup>*</sup>,
            <a href="http://wyang.me/">Wei Yang</a>,
            <a href="https://animesh.garg.tech/">Animesh Garg</a>, and
            <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a> <br />
          </div>
          <div class="paper-conf">RSS 2022 Workshop on Implicit Representations for Robotic Manipulation, 2022</div> 
          <font color="red">Spotlight Talk</font> <br />
          [<a href="https://arxiv.org/abs/2206.14854">Paper</a>]
          [<a href="">Project page</a>]
          [<a href="https://youtu.be/B-pEhT1pi-Q">Video</a>]
          [<a href="">Poster</a>]
          [<a href="papers/RSS-W-22/slides.pptx">Slides</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/CVPR-22/teaser.jpeg">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Neural Shape Mating: Self-Supervised Object Assembly with Adversarial Shape Priors</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="https://lihd1003.github.io/">Haoda Li</a>,
            <a href="http://www.cs.toronto.edu/~dylanturpin/">Dylan Turpin</a>,
            <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a>, and
            <a href="https://animesh.garg.tech/">Animesh Garg</a> <br />
          </div>
          <div class="paper-conf">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022</div> 
          [<a href="https://arxiv.org/abs/2205.14886">Paper</a>]
          [<a href="https://neural-shape-mating.github.io/">Project page</a>]
          [<a href="">Video</a>]
          [<a href="papers/CVPR-22/poster.pdf">Poster</a>]
          [<a href="">Slides</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/IROS-21/teaser.gif">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Learning by Watching: Physical Imitation of Manipulation Skills from Human Videos</div>
          <div class="paper-author"> 
            <a href="https://haoyu-x.github.io/">Haoyu Xiong</a>,
            <a href="https://quanzhou-li.github.io/">Quanzhou Li</a>,
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="https://homangab.github.io/">Homanga Bharadhwaj</a>,
            <a href="https://www.samsinha.me/">Samarth Sinha</a>, and
            <a href="https://animesh.garg.tech/">Animesh Garg</a> <br />
          </div>
          <div class="paper-conf">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021</div> 
          <div class="paper-conf">RSS 2021 Workshop on Visual Learning and Reasoning for Robotics, 2021 <font color="red">Spotlight Talk</font> <br /></div> 
          <div class="paper-conf">ICML 2021 Workshop on Human in the Loop Learning, 2021</div> 
          [<a href="https://arxiv.org/abs/2101.07241">Paper</a>]
          [<a href="http://www.pair.toronto.edu/lbw-kp/">Project page</a>]
          [<a href="https://youtu.be/Retu1q-BbEo">Video</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/PAMI-20/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Show, Match and Segment: Joint Weakly Supervised Learning of Semantic Matching and Object Co-segmentation</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>,
            <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, and
            <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a> <br />
          </div>
          <div class="paper-conf">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2021</div>
          [<a href="https://arxiv.org/abs/1906.05857">Paper</a>]
          [<a href="https://yunchunchen.github.io/MaCoSNet-web/">Project page</a>]
          [<a href="https://github.com/YunChunChen/MaCoSNet-pytorch">Code</a>]
          [<a href="papers/PAMI-20/slides.pdf">Slides</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/CVIU-21/teaser.gif">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Self-Attentive 3D Human Pose and Shape Estimation from Videos</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="https://mpicci.github.io/">Marco Piccirilli</a>,
            <a href="https://www.linkedin.com/in/rpiramuthu/">Robinson Piramuthu</a>, and
            <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
          </div>
          <div class="paper-conf">Computer Vision and Image Understanding (CVIU), 2021</div> 
          [<a href="https://arxiv.org/abs/2103.14182">Paper</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/ECCV-20/NAS-DIP/teaser.gif">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">NAS-DIP: Learning Deep Image Prior with Neural Architecture Search</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>,
            <a href="https://gaochen315.github.io">Chen Gao</a><sup>*</sup>,
            <a href="http://estherrobb.com">Esther Robb</a>, and
            <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a> <br />
          </div>
          <div class="paper-conf">European Conference on Computer Vision (ECCV), 2020</div>
          [<a href="https://arxiv.org/abs/2008.11713">Paper</a>]
          [<a href="https://yunchunchen.github.io/NAS-DIP/">Project page</a>]
          [<a href="https://github.com/YunChunChen/NAS-DIP-pytorch">GitHub</a>]
          [<a href="https://colab.research.google.com/drive/1BhmZMeyGGP_T5SLPdLlkGUZLhrnO1FdF?usp=sharing">Colab</a>]
          [<a href="papers/ECCV-20/NAS-DIP/highlight.mp4">Highlight video</a>]
          [<a href="papers/ECCV-20/NAS-DIP/highlight.pdf">Highlight slides</a>]
          [<a href="papers/ECCV-20/NAS-DIP/full.mp4">Full video</a>]
          [<a href="papers/ECCV-20/NAS-DIP/full.pdf">Full slides</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/ECCV-20/Meta/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Learning to Learn in a Semi-Supervised Fashion</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="">Chao-Te Chou</a>, and
            Yu-Chiang Frank Wang <br />
          </div>
          <div class="paper-conf">European Conference on Computer Vision (ECCV), 2020</div>
          [<a href="https://arxiv.org/abs/2008.11203">Paper</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/arXiv-20/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Cross-Resolution Adversarial Dual Network for Person Re-Identification and Beyond</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>,
            <a href="https://yujheli.github.io">Yu-Jhe Li</a><sup>*</sup>,
            <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>, and
            Yu-Chiang Frank Wang <br />
          </div>
          <div class="paper-conf">arXiv preprint arXiv:2002.09274</div>
          [<a href="https://arxiv.org/abs/2002.09274">Paper</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/ICCV-19/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>,
            <a href="https://yujheli.github.io">Yu-Jhe Li</a><sup>*</sup>,
            <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>,
            Xiaofei Du, and
            Yu-Chiang Frank Wang <br />
          </div>
          <div class="paper-conf">IEEE International Conference on Computer Vision (ICCV), 2019</div>
          [<a href="https://arxiv.org/abs/1908.06052">Paper</a>]
          [<a href="papers/ICCV-19/slides.pptx">Slides</a>]
          [<a href="papers/ICCV-19/poster.pdf">Poster</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/CVPR-19/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a>,
            <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, and
            <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a> <br />
          </div>
          <div class="paper-conf">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019</div>
          [<a href="https://arxiv.org/abs/2001.03182">Paper</a>]
          [<a href="https://yunchunchen.github.io/CrDoCo/">Project Page</a>]
          [<a href="https://github.com/YunChunChen/CrDoCo-pytorch">Code</a>]
          [<a href="papers/CVPR-19/slides.pptx">Slides</a>]
          [<a href="papers/CVPR-19/poster.pdf">Poster</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/AAAI-19/teaser.png">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Learning Resolution-Invariant Deep Representations for Person Re-Identification</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b><sup>*</sup>,
            <a href="https://yujheli.github.io">Yu-Jhe Li</a><sup>*</sup>,
            Xiaofei Du, and
            Yu-Chiang Frank Wang <br />
          </div>
          <div class="paper-conf">AAAI Conference on Artificial Intelligence (AAAI), 2019</div>
          <font color="red">Oral Presentation</font> <br />
          [<a href="https://arxiv.org/abs/1907.10843">Paper</a>]
          [<a href="papers/AAAI-19/slides.pptx">Slides</a>]
          [<a href="papers/AAAI-19/poster.pdf">Poster</a>]
        </div>
      </div>

      <div class="row paper-block">
        <div class="col s12 m12 l4 center">
          <img class="responsive-img" src="papers/ACCV-18/teaser.jpg">
        </div>
        <div class="col s12 m12 l8">
          <div class="paper-title">Deep Semantic Matching with Foreground Detection and Cycle-Consistency</div>
          <div class="paper-author"> 
            <b><a href="https://yunchunchen.github.io">Yun-Chun Chen</a></b>,
            Po-Hsiang Huang,
            Li-Yu Yu,
            <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>,
            <a href="http://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, and
            <a href="https://sites.google.com/site/yylinweb/">Yen-Yu Lin</a> <br />
          </div>
          <div class="paper-conf">Asian Conference on Computer Vision (ACCV), 2018</div>
          [<a href="https://arxiv.org/abs/2004.00144">Paper</a>]
          [<a href="https://yunchunchen.github.io/WeakMatchNet/">Project Page</a>]
          [<a href="https://github.com/YunChunChen/WeakMatchNet">Code</a>]
          [<a href="papers/ACCV-18/poster.pdf">Poster</a>]
        </div>
      </div>

  </div>
</div>

<footer class="page-footer white lighten-4">
    <div class="row">
      <div class="col l4 offset-l4 s12">
        <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=THulLRlsDUvjodnYpDCviW_mAVbauRC9pS7yOCczJQI"></script>
      </div>
    </div>
    <div class="footer-copyright center black-text">
      Template from <a href="http://www.wslai.net">Jason Lai</a>
    </div>
  
</footer>

<script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="js/materialize.js"></script>
<script src="js/aos.js"></script>
<script src="js/init.js"></script>

</body>
</html>
